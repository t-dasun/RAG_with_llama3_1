{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7137cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# !pip install -qU transformers torch accelerate bitsandbytes\n",
    "# !pip install -qU langchain langchain_community langchain_huggingface\n",
    "# !pip install -qU sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_knowledge.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create our Knowledge Base \n",
    "# create example txt file as our external knowledge source\n",
    "#\n",
    "%%writefile my_knowledge.txt\n",
    "The Aethelred Operating System, first released in 2023, is a lightweight, security-focused OS designed for embedded systems.\n",
    "Its core feature is the \"Chrono-Lock\" sandbox, which isolates every application in a time-sensitive, encrypted container. This prevents unauthorized data access and rollback attacks.\n",
    "Aethelred OS does not support traditional graphical user interfaces (GUIs); it is managed entirely through a command-line interface (CLI) called the \"A-Shell\".\n",
    "The primary programming language for Aethelred development is \"Veridian,\" a memory-safe language similar to Rust, but with built-in primitives for asynchronous hardware communication.\n",
    "The latest version, Aethelred 3.1 \"Dragon-Tooth,\" introduced support for multi-core processing, a feature that was absent in earlier versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73bd9cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 1 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12009/2713796121.py:23: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/home/dasun/venvs/ai_test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Phase 1 - Indexing our Document ðŸ“š\n",
    "\n",
    "# Now, we'll load the document, split it into manageable chunks, create numerical embeddings for each chunk, \n",
    "# and store them in a searchable FAISS vector database.\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 1. Load the document\n",
    "loader = TextLoader(\"./my_knowledge.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 2. Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Split document into {len(chunks)} chunks.\")\n",
    "\n",
    "# 3. Load a sentence-transformer model for embeddings\n",
    "# This model is small, fast, and runs locally.\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# 4. Create the FAISS vector store\n",
    "# This command creates embeddings for the chunks and stores them.\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "print(\"Vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b783348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (0.34.4)\n",
      "Requirement already satisfied: filelock in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from huggingface_hub) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dasun/venvs/ai_test/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210076ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Replace \"YOUR_HF_TOKEN\" with your actual token\n",
    "# login(\"YOUR_HF_TOKEN\")\n",
    "hf_token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1647e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.28s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain created and ready to use.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Phase 2 - Setting up the RAG Chain\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Hugging Face Hub Login (if not already done in terminal)\n",
    "# from huggingface_hub import login\n",
    "# login(\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# Define the model ID for Llama 3.1 8B Instruct\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"unsloth/llama-3.1-8b-bnb-4bit\"\n",
    "\n",
    "# Use quantization to reduce memory usage\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    top_p=0.9,\n",
    "    temperature=0.1, # Use low temperature for factual answers\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a LangChain object\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Define the RAG prompt template\n",
    "prompt_template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a helpful assistant. Please answer the user's question based only on the following context. If the answer is not in the context, say you don't know. Do not use any prior knowledge.\n",
    "\n",
    "CONTEXT:\n",
    "{context}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "QUESTION:\n",
    "{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# Create the retriever from our vector store\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 chunks\n",
    "\n",
    "# Create the RAG chain using LangChain Expression Language (LCEL)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"RAG chain created and ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a49db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(raw_output: str) -> str:\n",
    "    if \"assistant<|end_header_id|>\" in raw_output:\n",
    "        return raw_output.split(\"assistant<|end_header_id|>\")[-1].strip()\n",
    "    return raw_output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ce8b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the primary programming language for Aethelred OS?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The primary programming language for Aethelred OS is \"Veridian\", a memory-safe language similar to Rust, but with built-in primitives for asynchronous hardware communication.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: Who is the CEO of the company that makes Aethelred OS?\n",
      "Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "# Question 1: The answer is in the document.\n",
    "question1 = \"What is the primary programming language for Aethelred OS?\"\n",
    "print(f\"Question: {question1}\")\n",
    "\n",
    "# Invoke the chain\n",
    "# answer1 = rag_chain.invoke(question1)\n",
    "answer1 = clean_answer(rag_chain.invoke(question1))\n",
    "print(f\"Answer: {answer1}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Question 2: The answer is NOT in the document.\n",
    "question2 = \"Who is the CEO of the company that makes Aethelred OS?\"\n",
    "print(f\"Question: {question2}\")\n",
    "\n",
    "# Invoke the chain\n",
    "answer2 = clean_answer(rag_chain.invoke(question2))\n",
    "print(f\"Answer: {answer2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
